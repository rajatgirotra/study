Consistent hashing is used to solve the problem of efficiently distributing requests/data across servers. Consider you have 4 servers s0, s1, s2, s3 which are used to store data evenly. To do that, you use a hashing function on the data % the count of the servers. So lets say your data is now split amongst the 4 servers. If s0 goes down, you need to again re-map all your data by rehashing it again amongst the three servers. This leads to almost all the data getting remapped to a different server and a lot of movement has to take place. THis is what consistent hashing tries to solve. 

Lets say you hash function is SHA-1 which generates hash keys from 0 to 2^160-1. i.e. a very large data set. Think of this large hash set as a ring. Along this ring, you place the 4 servers. Now every data is hashed to some point on the ring. From that point, you keep going clockwise until you find a server. It is that server which is used to store/retrieve the data.

If a server goes down, only the data on that server is effected and needs to be moved to the next server. If a server is added, only the data between the newly added server and the previous server needs to be moved to the newly added server. Basically consistent hashing minimizes this movement of data. It guarantees that at most k/n keys will have to moved, where k is the count of the hashing keys and n is the number of buckets/servers/slots.

A problem with consistent hashing is that the server might be placed unevenly along the ring. There could be a situation where a server has too many or too few hash keys mapped to it.
To get around this problem, we use virtual nodes. For example, we could have servers s0_0, s0_1, s0_2 and s1_0 mapped to server 0, s1_1, s1_2 all of which map to server 1. As the number of virtual nodes increases, the distribution of keys becomes more balanced. This is because the standard deviation gets smaller with more virtual nodes, leading to balanced data distribution.
