Over the years with the increasing use of electronic gadgets, there is tremendous amount of data generated in various forms. And the data being generated is increasing exponentially. Examples

1) The biggest chunks of data are generated by social media sites like Facebook, Twitter, LinkedIn, Instagram, Youtube in forms of like , shares, comments, posts. If we research the amount of likes generated every 60 seconds, the amount of videos uploaded every 60 seconds, the shares, and messages, documents etc being shared on Whatsapp; we can realise the data is mammoth.

2) Smart phones are generating so much data in terms of logs/application usage and patters etc etc.

3) We have internet of things. ie smart electronic devices like Aircon, Refrigerators etc which have Wi-fi and they also generate data.

All this data is un-structured or semi-structured, ie the data is heterogenous. And the volume of data is so large that we can not use the traditional method of processing data by inserting them in traditional RDBMS systems and performing analytics on it and generating useful results which draw business actions. So really big_data can be identified using

1) The sheer Volume of data being generated
2) The structure of data (big data is un-structured, semi-structured ie heterogenous).
3) The velocity (speed) with which it is increasing.

So now the problem is how to we accumulate, process, analyse and generate useful results out of it.
