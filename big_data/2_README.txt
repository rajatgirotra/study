Hadoop in Big Data:
------------------ 
How hadoop comes in?

Bob opens a small restaurant with one Chef and one waitor. The Chef gets roughly 2 orders every hour; and he is easily able to cook 2 dishes every hour. The Chef also has a shelf of raw-materials to choose from during cooking. Now Bob decides to takes online orders and suddenly the Chef has to cook 10 dishes every hour. That is too much to ask for. So what is the simple solution. 

HIRE MORE CHEFS... :)

1) So chefs really is nothing but compute power. ie a data processing unit.
Now if Bob hires 4 more chefs (total 5 now), they all contend for the same shelf for raw materials. So the bottleneck is now the shelf. ie access to data is a bottleneck. So what do we do? Simple:

GET MORE SHELVES, may be one for each chef. Cool. 
2) Shelves here is the data to be processed. So you have to duplicate the data across all your compute nodes.

3) Next if some-one wants meat-sauce. You can have one chef who can cook meat, one chef who can cook sauce at the same time, and one head-chef who can put them together and server it. So what are we doing now? 
PARALLELIZATION..

That's what Hadoop is: Hadoop is an eco-system for processing large and complex data sets, by using distributed and parallel computing.


