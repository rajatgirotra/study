{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0632d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0261e64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "pl.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be the raw download page link\n",
    "!wget https://raw.githubusercontent.com/mattharrison/2023-Pycon-Polars/refs/heads/main/__mharrison__2020-2021.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75097300",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '__mharrison__2020-2021.csv'\n",
    "df = pl.read_csv(file)\n",
    "\n",
    "# or read it directly from url\n",
    "url = 'https://raw.githubusercontent.com/mattharrison/2023-Pycon-Polars/refs/heads/main/__mharrison__2020-2021.csv'\n",
    "df = pl.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d259f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3538b65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.schema # sees column names and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfea8f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe() # some metadata about your dataframe. Not all the information may be of importance. but it does tell\n",
    "# you the number of nulls in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.quantile(0.25) # get the row at the 25% quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01032aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'first row\\n {df[0]}') # get first row\n",
    "print(f'second row\\n {df[1]}') # get second row\n",
    "print(f'row 1-4\\n {df[1:5]}') # get first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9f58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(pl)) # see things available in pl package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5916a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# polars is written from the ground up. Under the python code in polars is a Rust layer.\n",
    "# to get the speed and performance benefits, polars introuced \"expressions\" i.e polars.expr.expr.Expr. Expressions are just a way to describe\n",
    "# what you intend to do. It is just a way to hold your execution plan. Expressions don't hold any data.\n",
    "# so Expressions enable deferred computation — they are a recipe for how to compute something on a DataFrame or Series.\n",
    "# You can think of it as a lazy formula that Polars will evaluate later when needed. Polars uses expressions to build efficient,\n",
    "# vectorized, and parallel query plans — much like SQL or Spark.\n",
    "\n",
    "# These expressions can be either evaluated eagerly or lazily. I will show later how we can create lazy data frames and inspect our evaluation\n",
    "# plan using api's such as \"describe_plan(), describe_optimized_plan(), and explain()\"\n",
    "\n",
    "# if you use the python layer, yes you get some flexibility but it comes at the expense of speed.\n",
    "\n",
    "print(type(pl.col('foo'))) # pl.col('foo') is polars.expr.expr.Expr\n",
    "\n",
    "# You can do many things with expressions we will see later but lets just print the dir of the expressions.\n",
    "print(dir(pl.col('foo'))) # see things available in polars columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce7053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index # no index in polars by default. need to use with_row_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b63fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e5771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(20).transpose() # just a way of seeing some initial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes # these are pyarrow types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2246f8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.estimated_size() # rough bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1:5] # print rows 1, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e5a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prefer to use Expressions instead of working directly on dataframe\n",
    "# use pl.all() instead of pl.col('*')\n",
    "\n",
    "# Mostly you will be using 4 common apis in polars. we will see them in action next.\n",
    "# select(), with_columns(), filter(), groupby().agg()\n",
    "\n",
    "# df.select() is used to choose columns (you can give it multiple expressions as positional arguments, or even keyword expressions as \n",
    "# keyword arguments. the keys you use become the name(alias) for your columns\n",
    "df.select(pl.all()) # get all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a52e764-552e-44bb-ad68-4a0c929021bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(pl.col('Tweet id').alias('Rajat')).head() # select only one column and change its column name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9864df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use select to get all columns of float64 type\n",
    "df.select(pl.col(pl.Float64)).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25820ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.col/pl.all is very flexible. Think of it as a where clause in a select statement.\n",
    "# exclude coloumns using exclude function, apply regex also on columns filtering. See examples below\n",
    "df.select(pl.all().exclude('Tweet id')) # select and where clause\n",
    "\n",
    "# pl.exclude is just a syntactic sugar over pl.all().exclude()\n",
    "df.select(pl.exclude(\"^Tweet\")) # remove all columns that begin with Tweet\n",
    "\n",
    "df.select(pl.exclude([pl.Float64])) # exclude Float64 columns. exclude can take a list\n",
    "\n",
    "df.select([\"impressions\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d57a0-7355-4502-92e0-42a024db8e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.select(['impressions', 'impressions']) # polars doesnt allow duplicate columns\n",
    "\n",
    "# Workaround\n",
    "df.select(['impressions', pl.col('impressions').alias('impressions_2')]) # polars doesnt allow it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6418be44-2e25-4262-b3e5-f405a7859782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a column where all cell values are \"0 if impressions is less than 100, or else 1\"\n",
    "expr = (pl.when(pl.col('impressions') < 100).then(0).otherwise(1))\n",
    "new_df = df.select(expr.alias('threshold'))\n",
    "print(new_df)\n",
    "print(new_df.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde4654-1f1b-4b17-a5b4-0270c8a241c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming and replacing columns is really trivial.\n",
    "\n",
    "# renaming - Just use df.rename() method\n",
    "test_df = pl.DataFrame({\"foo\": [1, 2, 3], \"bar\": [6, 7, 8], \"ham\": ['a', 'b', 'c']})\n",
    "print(test_df)\n",
    "print(test_df.rename({'foo': 'apple'})) # just a dict of old vs new names\n",
    "print(test_df.rename(lambda x : \"c\" + x[1:])) # a lambda to rename all columns\n",
    "\n",
    "# replacing\n",
    "apples = pl.Series(\"apple\", [10, 20, 30])\n",
    "print(test_df.replace_column(0, apples)) # replace column at position 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec4b4ce-2d4f-4e0f-b3be-b5a0e6258668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that most operations return a copy of the dataframe and do not modify the original dataframe\n",
    "# with_columns(pl.lit('Q0).alias('Describe')) # with_columns means add a new column, pl.lit('Q0') means add a literal column, i.e. a column with Q0 as data.\n",
    "# and rename it to Describe\n",
    "df.quantile(0).with_columns(pl.lit('Q0').alias('Describe')).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494bf3c4-7599-4223-a690-f9dc0d3f2000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add describe column at the beginning instead\n",
    "\n",
    "# DeprecationWarning: `NUMERIC_DTYPES` was deprecated in version 1.0.0. Define your own data type groups or use the `polars.selectors` module\n",
    "# for selecting columns of a certain data type.\n",
    "df = df.select([\n",
    "    pl.lit('Q1').alias('Describe'),\n",
    "    pl.col(pl.NUMERIC_DTYPES).quantile(0)\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67368fb-ec24-4f99-be31-c58edfc100e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above but using selectors\n",
    "# selectors allow for more intuitive selection of columns from DataFrame or LazyFrame. They extend on the pl.col() functionality.\n",
    "import polars.selectors as cs\n",
    "\n",
    "df = df.select([\n",
    "    pl.lit('Q0').alias('Describe'),\n",
    "    cs.by_dtype(pl.Float64).quantile(0)\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250be32-ef79-41b7-afc0-a28b52c04006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at df.describe() command. We want to add a new statistic of Q0, Q0.25, Q0.5, Q0.75, Q1\n",
    "df.describe() # may be initialize the dataframe again and then see the clean output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d176f-af98-48cb-90b8-23ff3e85a431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.quantile(0).with_columns(pl.lit(f'Q0').alias('Describe')).select(pl.col('Describe'), pl.exclude('Describe'))\n",
    "# debug statement\n",
    "  \n",
    "\n",
    "# how we can do that. we can use pl.concat to concat multiple dataframes.\n",
    "# lets fetch the Q0, Q0.25, Q0.5, Q0.75, Q1 quantiles rows and a new column \"Describe\" (at the front) with values Q0, Q0.25, Q0.5, Q0.75, Q1 \n",
    "# then lets fetch the df.describe() dataframe and just connect both. Easy peasy!!.\n",
    "\n",
    "# quantile(val) returns a dataframe with the exact same schema and structure as df.describe() so they can be concatenated together.\n",
    "(pl\n",
    "  .concat(\n",
    "    [ \n",
    "        # 1st dataframe\n",
    "        *[df\n",
    "        .quantile(val)\n",
    "        .with_columns(pl.lit(f'Q{val}').alias('statistic')) # problem is that Describe will be added at the end. We need to move it to the beginning\n",
    "        .select(pl.col('statistic'), pl.exclude('statistic'))\n",
    "        for val in [0, 0.25, 0.5, 0.75, 1]\n",
    "        ],\n",
    "        # 6th dataframe (1st to 5th dataframes are for the quantiles\n",
    "        df.describe() \n",
    "    ],\n",
    "    how='vertical'\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae57320-48ef-4769-93f7-263f081c035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets just put this into a function and call dataframe.pipe(myfunction)\n",
    "\n",
    "def pd_describe(a_df: pl.DataFrame, *args, **kwargs):\n",
    "    cols = ['statistic', *a_df.columns]\n",
    "    return (pl\n",
    "        .concat(\n",
    "            [ \n",
    "                # 1st dataframe\n",
    "                *[a_df\n",
    "                .quantile(val)\n",
    "                .with_columns(pl.lit(f'Q{val}').alias('statistic')) # problem is that Describe will be added at the end. We need to move it to the beginning\n",
    "                .select(cols)\n",
    "                for val in [0, 0.25, 0.5, 0.75, 1]\n",
    "                ],\n",
    "                # 6th dataframe (1st to 5th dataframes are for the quantiles\n",
    "                a_df.describe() \n",
    "            ],\n",
    "            how='vertical'\n",
    "          )\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b04031-b644-414b-a092-641ccc7b8bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.pipe offers a structured way to apply a sequence of user-defined functions (UDFs).\n",
    "df.pipe(pd_describe, 1, 2, 3, name='Rajat') # pd_describe receives df as the a_df argument. The rest of the arguments 1, 2, 3 and name are passed as *args and **kwargs\n",
    "\n",
    "# pd_describe function is a very handy way to look at statistics for any partial data in your data frame.\n",
    "# you could select just all the I64 columns and call pipe on it and it will give you statistics for that partial data. See example below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e7a342-a6f3-43bd-a503-2e6b199d9692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.pipe offers a structured way to apply a sequence of user-defined functions (UDFs).\n",
    "df.select(pl.col(pl.Float64)).pipe(pd_describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39476240-766a-451f-a947-bdbe0b12ffaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just like you call pipe() on a dataframe, you can call pipe on a pl.Expr object.\n",
    "# and just like DataFrame.pipe() returns a DataFrame, calling pipe() on pl.Expr should return another pl.Expr.\n",
    "\n",
    "# Lets say you have a dataFrame with one column where values are \"a: 1\", \"b: -2\", \"c: 3\", \"d: -4\".\n",
    "# try to write a pipe based function which extracts the integer out and creates a new col based on it.\n",
    "# if it is even. change to negative, otherwise leave it untouched. then multiply it by 5. so result should be 5, -10, 15. -20\n",
    "\n",
    "# we will create two User defined functions which will take a pl.Expr and return it to achieve the above.\n",
    "temp_df = pl.DataFrame({'val': [\"a: 1\", \"b: -2\", \"c: 3\", \"d: -4\"]})\n",
    "temp_df\n",
    "\n",
    "def extract_int_from_str(col: pl.Expr) -> pl.Expr:\n",
    "    return col.str.extract(r\"\\d+\", 0).cast(pl.Int64)\n",
    "\n",
    "def scale_negative_even(expr: pl.Expr, k: int=5) -> pl.Expr:\n",
    "    expr = pl.when(expr % 2 == 0).then(-expr).otherwise(expr)\n",
    "    return expr * k\n",
    "\n",
    "temp_df.with_columns(udfs=pl.col('val').pipe(extract_int_from_str).pipe(scale_negative_even, 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a554e-af37-4d26-a9ca-dc199c368b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# casting\n",
    "df.select(pl.col('impressions').cast(pl.Int32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049dc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stick the new Int32 impressinos column back in the main dataframe\n",
    "df.with_columns(pl.col('impressions').cast(pl.Int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64208a5-590f-491a-ac30-18cf12c385af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f6852-862b-4c06-b2c0-d6694586fc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.iinfo(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562c991c-f6f0-4694-8c6d-6e10397cc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(cs.numeric()).columns # cs.numeric means all integer and floating types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44aaa66-2749-461f-bbb0-0e11493665ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "(df\n",
    " .select(cs.numeric())  # take all the numeric cols\n",
    " .pipe(pd_describe) # do a describe on that\n",
    " .select([ # select numeric cols where max value is <= 255\n",
    "     pl.col(col)\n",
    "       for col in df.select(cs.numeric()).columns\n",
    "     if df[col].max() <= 255 \n",
    " ])\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9ae24-76f7-4035-8381-84fb8445676e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(pl.col('Tweet id')).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b5e76-7cff-42a5-b24a-8ede575feb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laziness\n",
    "# in polars, you can actually create a chain of User defined operations on your dataframe and then hand it together to polars\n",
    "# polars will then analyze it, optimize it and run it as one big operations instead of running many smaller un-optimized operations\n",
    "\n",
    "lf = df.lazy() # convert you dataframe to a lazy dataframe\n",
    "res = lf.with_columns((pl.col(\"Tweet id\") + 10).alias('Tweet id + 10')) # this wont return any data unless you call collect on it.\n",
    "print(type(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa25339-a115-4607-a1f2-644ee1902c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.collect().select(['Tweet id', 'Tweet id + 10']).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfa2e7-5df4-40e9-99b9-6de6e6e38282",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pl.DataFrame({\n",
    "    \"x\": [1, 2, 3],\n",
    "    \"y\": [10, 20, 30]\n",
    "})\n",
    "print(temp_df)\n",
    "# create a new column which is a square of the element in the first column plus the value in second column\n",
    "def squared(expr: pl.Expr) -> pl.Expr:\n",
    "    return expr * expr\n",
    "\n",
    "# 1st way, use pipe() api\n",
    "print(temp_df.with_columns((pl.col('x').pipe(squared) + pl.col('y')).alias('new_value')))\n",
    "\n",
    "# 2nd way, use map_elements() which takes a lambda\n",
    "# Expr.map_elements is significantly slower than the native expressions API above.\n",
    "# Only use if you absolutely CANNOT implement your logic otherwise.\n",
    "print(temp_df.with_columns((pl.col('x').map_elements(lambda c : c**2) + pl.col('y')).alias('new_value')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c02f1c-6d25-45e9-974b-25a545fab5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how groupby() and aggregation() works?\n",
    "temp_df = pl.DataFrame({\n",
    "    \"team\": [\"A\", \"A\", \"B\", \"B\"],\n",
    "    \"score\": [5, 7, 6, 8]\n",
    "})\n",
    "\n",
    "# groupby \"team\" and calculate the \"total score\"\n",
    "print(temp_df.group_by('team').agg(pl.col('score').sum())) # the agg() api gets the score column as a pl.Series and applies sum() on it\n",
    "\n",
    "# groupby \"team\" and calculate the \"difference in max and min scores\"\n",
    "# print(temp_df.group_by('team').agg(pl.col('score').apply(lambda score_series: score_series.max() - score_series.min()).alias('score_range'))) \n",
    "\n",
    "# groupby \"team\" and accumulate the values in y and also calculate their mean.\n",
    "print(temp_df.group_by('team').agg([pl.col('score').alias('all_scores'), pl.mean('score').alias('scores_mean')]).sort(by=pl.col('team')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3561e74-7d08-4530-8c81-1d520d454726",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pl.DataFrame({\"x\": {\"a\": 1, \"b\": 2}})\n",
    "print(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11dc10f-2cbd-4818-bd82-f0cafd1f5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is above? You can create DataFrame and Series where data type is struct. A struct contains fields. Each field is identified by pl.Field() object\n",
    "# which has a name and an associated data type. Lets create a Struct type using a list of fields or a map/dict of field name and types.\n",
    "from typing import List\n",
    "import datetime\n",
    "s1 = pl.Struct([pl.Field('a', pl.Int64), pl.Field('b', pl.Datetime), pl.Field('c', pl.List(pl.String))])\n",
    "s2 = pl.Struct({'a': pl.Int64, 'b':pl.Datetime, 'c': pl.List(pl.String)})\n",
    "# s1 and s2 are same\n",
    "print(type(s1))\n",
    "\n",
    "temp_series = pl.Series([\n",
    "                         {'a': 1, 'b': datetime.datetime.now(), 'c': ['Rajat', 'Vidhu']},\n",
    "                         {'a': 2, 'b': datetime.datetime(2025, 10, 10), 'c': ['Raman', 'Aashish']}\n",
    "                        ], dtype=s1)\n",
    "print(temp_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d477dea-8865-4140-b714-e5724b1f61e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pl.DataFrame(temp_series)\n",
    "print(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386e3e5-3cbd-4b3b-91c3-6dfc21e78503",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
