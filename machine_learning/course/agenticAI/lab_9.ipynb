{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are going to build a very trivial langgraph application. An application which just does the five steps we explained in 6_README.txt, i.e.\n",
    "### Define a State class, create a graph builder, create nodes and egdes, compiler and finally run the graph.\n",
    "\n",
    "### the nodes wont even do anything really agentic. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END  # StateGraph is the graph_builder object, START and END are used to create when creating the starting and ending node of your workflow.\n",
    "from langgraph.graph.message import add_messages # add_message is a very trivial reducer which takes two lists and concatenates the two lists.\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pydantic import BaseModel, Field\n",
    "import random\n",
    "from IPython.display import Image, display, Markdown\n",
    "from typing import Annotated\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a list of nounes and adjectives and when we create the node later on, it will randomly pick one noun and one adjective and return a new State object with the new noun and adjective.\n",
    "nouns = [\n",
    "    \"mountain\", \"river\", \"forest\", \"computer\", \"ocean\",\n",
    "    \"book\", \"planet\", \"castle\", \"robot\", \"guitar\",\n",
    "    \"bridge\", \"car\"\n",
    "]\n",
    "\n",
    "# List of random adjectives\n",
    "adjectives = [\n",
    "    \"ancient\", \"massive\", \"silent\", \"glowing\", \"fragile\",\n",
    "    \"noisy\", \"colorful\", \"mysterious\", \"swift\", \"brilliant\",\n",
    "    \"dark\", \"icy\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side note on how langgraph uses \"Annotated\" to install reducers. \n",
    "### You already know about python type hints that you can apply to arguments or function return type etc. Example:\n",
    "\n",
    "```python\n",
    "def shout(text: str) -> None:\n",
    "    print(text.upper())\n",
    "```\n",
    "\n",
    "### Another way of specifying type hints is using the \"Annotated\" syntax.\n",
    "```python\n",
    "def shout(text: Annotated[str, \"A text to print in upper case\"]) -> None:\n",
    "    print(text.upper())\n",
    "```\n",
    "\n",
    "### Annotated description inside the Annotated expression is not even parsed by the python interpreter. It simply discards it. However, langgraph uses it.\n",
    "### the Annotated expression is used to specify the reducer. and we will use the \"add_messages\" reducer next.\n",
    "\n",
    "### the add_messages reducer is a built-in reducer function thatâ€™s primarily used in chat-style applications. Its job is to accumulate (append) the messages exchanged between the user and the system over the course of the graph's execution. It adds messages returned by each node (usually in OpenAI-style {\"role\": ..., \"content\": ...} format) to the messages list in the graph's state. This helps maintain a chat history or conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create the State class next. It could either by a pydantic class or a typed Dict. We will use pydantic.\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph builder object.\n",
    "graph_builder = StateGraph(State)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a node function. Remember that the node function takes a state and returns a state. and the state is immutable.\n",
    "def our_first_node(old_state: State) -> State:\n",
    "    reply = f\"{random.choice(nouns)} are {random.choice(adjectives)}\"\n",
    "    # we create a json object, similar to openAI messages.\n",
    "    messages = [\n",
    "        {\"role\": \"assistant\", \"content\": reply}\n",
    "    ]\n",
    "    return State(messages=messages)\n",
    "\n",
    "# create a node.\n",
    "node = graph_builder.add_node(\"first_node\", our_first_node)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create edges with just the single nodes.\n",
    "graph_builder.add_edge(START, \"first_node\")\n",
    "graph_builder.add_edge(\"first_node\", END)\n",
    "\n",
    "# compile the graph.\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the graph.\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally create the gradio chat interface. Remember that the chat interface takes a current message and the history.\n",
    "# In the chat, we want to invoke our graph as it is already compiled and ready to use. We really dont make any LLM call just yet.\n",
    "def chat(user_input: str, history):\n",
    "    message = {\"role\": \"user\", \"content\": user_input} # this is the message we will give to the graph.\n",
    "    messages = [message]\n",
    "    state = State(messages=messages)\n",
    "    result = graph.invoke(state)\n",
    "    print(result)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "\n",
    "# What really is happening:\n",
    "# 1.A user enters a message.\n",
    "# 2.The message is added to the state.\n",
    "# 3.The state is passed to the graph.\n",
    "# 4.The graph invokes the node function.\n",
    "# 5.The node function returns a state.\n",
    "# 6.The reducer function(s) is called with the old state and the new state.\n",
    "# 7.The reducer function(s) update the state based on the node function's output.\n",
    "# 8.The updated state is passed to the next node function.\n",
    "# 9.This continues until the graph reaches the END node.\n",
    "# 10.The final state is returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you really see, langgraph is all about python functions - it doesnt need to invoke LLMs!!\n",
    "### Next, lets do all steps above again, but this time we will use a LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "def chatbot_node(old_state: State) -> State:\n",
    "    response = llm.invoke(old_state.messages)\n",
    "    new_state = State(messages=[response])\n",
    "    return new_state\n",
    "\n",
    "graph_builder.add_node(\"chatbot_node\", chatbot_node)\n",
    "graph_builder.add_edge(START, \"chatbot_node\")\n",
    "graph_builder.add_edge(\"chatbot_node\", END)\n",
    "\n",
    "graph = graph_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(user_input: str, history):\n",
    "    message = {\"role\": \"user\", \"content\": user_input} # this is the message we will give to the graph.\n",
    "    messages = [message]\n",
    "    state = State(messages=messages)\n",
    "    result = graph.invoke(state)\n",
    "    print(result)\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "\n",
    "# In your chat, tell the chat interface your name first and then ask the chatbot if it knows your name.\n",
    "# It wont remember that as we are not using any memory. In fact the history argument is unused."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
