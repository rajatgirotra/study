{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing DataSet\n",
    "dataset = pd.read_csv('MachineLearningAZ/Part1_DataPreprocessing/Section2_Part1_DataPreProcessing/Python/Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use iloc to separate features and labels\n",
    "X = dataset.iloc[:, :-1].values  # iloc is used to address DataFrame using rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataset.iloc[:, -1].values\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing data\n",
    "# 1. Either remove the complete row, which could be very dangerous as you might remove very important training data in the other columns\n",
    "# 2. Replace missing data by the mean of the data for that column We'll do step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SimpleImputer class provides basic strategies for imputing missing values, like\n",
    "# using the mean of the column or row values\n",
    "# using the median of the column or row values\n",
    "# using the mode of the column or row in which the missing values are located.\n",
    "# using a constant value\n",
    "# This class also allows for different missing values encodings.\n",
    "imputer = SimpleImputer(missing_values=np.NaN, strategy='mean')\n",
    "# Apply the imputer using the fit function.\n",
    "# Here the fit method, when applied to the training dataset,learns the model parameters (for example, mean and standard deviation).\n",
    "# We then need to apply the transform method on the training dataset to get the transformed (scaled) training dataset.\n",
    "# We could also perform both of this steps in one step by applying fit_transform on the training dataset.\n",
    "# Then why do we need 2 separate methods - fit and transform ?\n",
    "# In practice we need to have a separate training and testing dataset and that is where having a separate fit and transform method helps.\n",
    "# We apply fit on the training dataset and use the transform method on both - the training dataset and the test dataset.\n",
    "# Thus the training as well as the test dataset are then transformed(scaled) using the model parameters that were learnt on applying\n",
    "# the fit method the training dataset.\n",
    "imputer = imputer.fit(X[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally impute using transform, transform also takes a parameter of what to impute. impute(mean, median, mode) values \n",
    "# are already computed at the fit() step\n",
    "# X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to encode categorical data to quantitative data ie columns like Country and Purchased.\n",
    "# Machine Learning algo's are based on equations so all categorical data needs to be converted to quantitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelEncoder is very trivial. Just a simple class to give numeric values from 0 to n-1 (where there are n different\n",
    "# label types).\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_X = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the labels, ie let the LabelEncoder know about the labels\n",
    "le_X.fit(X[:, 0])\n",
    "\n",
    "# View the labels\n",
    "print(le_X.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now transform, xform will return an array of the numeric values from 0 to n-1 assigned to each label.\n",
    "le_X.transform(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse_transform\n",
    "le_X.inverse_transform([0, 2, 1, 2, 1, 0, 2, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also fit and transform in the same step. Infact a lot of classes have the fit(), tranform() and fit_transform()\n",
    "# api's available. You can use fit_transform(X, y=None), in cases where the data to fit and the data to xform are the\n",
    "# same; or both are known before hand.\n",
    "le_X = LabelEncoder()\n",
    "X[:, 0] = le_X.fit_transform(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big Problem Above, The countries values are encoded as 0, 1, 2. This will make ML algo's think that these values \n",
    "# are numerically comparable, ie something like 0(France) is less than 2(Spain) and 1(Germany) is greater than 0(France).\n",
    "# If these were some other categorical data like size of shirts (Small, Medium, Large), it would have made sense, but\n",
    "# here it doesnt make sense. To solve this, after we convert categorical data to quantitative, we have to separate the\n",
    "# Countries into 3 columns (ie into as many columns as many categories of this feature. The value in these three \n",
    "# column will be binary 0 or 1. So wherever the original country is France, the France column will have value 1 or \n",
    "# other columns will have value 0. We use another Encoder for that next called OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infact we dont use OneHotEncoder directly, we use the ColumnTransformer class which is a high level class that can\n",
    "# transform multiple categorical data based on different tranformers you give\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "X = column_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets encode the label to also quantitative data. We dont need OneHotEncoder here as labels are unrelated\n",
    "# so it never compares them.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_y = LabelEncoder()\n",
    "y = le_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets split the data set into training and test set\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8) # can give random_state argument to fix the train/test set indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling - VERY VERY IMPORTANT LESSON. \n",
    "# In ML, its very important that all features be on the same scale (range). \n",
    "# this way ML algo will give equal weight to all features. \n",
    "# Here we have age and salaries on different scales\n",
    "print('Age range %s to %s' % (dataset['Age'].min(), dataset['Age'].max()))\n",
    "print('Salary range %s to %s' % (dataset['Salary'].min(), dataset['Salary'].max()))\n",
    "\n",
    "# A number of ML algos are dependent on the euclidian distance (ED) between two points. ED is the square root of\n",
    "# summation of squares of the distance between the x points and the y points. So if we draw a plot with age on the\n",
    "# x-axis and salary on the y-axis, the range of square of salaries will be even higher then the range of square of ages\n",
    "# so it will be almost like the age's are non-existant compared to salaries. So its very important that they be in the\n",
    "# same scale. By scaling we bring all features in the range like -1 to 1 or 0 to 1 (like that).\n",
    "\n",
    "# Two methods of scaling exist:\n",
    "# 1. Standardization - i.e. each value is changed by it's z-score\n",
    "# 2. Normalization - i.e each value is reduced to (x-x.min()/(x.max()-x.min()))\n",
    "# Lets attempt to do feature scaling the age and salary now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again for feature scaling, we use StandardScalar from sklearn preprocessing module\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:, 3:] = sc_X.fit_transform(X_train[:, 3:])\n",
    "X_test[:, 3:] = sc_X.transform(X_test[:, 3:])  # no need to fit here as we already fitted based on training data above.\n",
    "\n",
    "# A good question is whether or not we should feature scale the dummy variables (0,1) from the OneHotEncoder(). The\n",
    "# answer is that it always depends on context. It depends on how much interpretation you want in your model. in our case\n",
    "# if we scale the dummy variables, then we loose the meaning of which country does a value like 0.5 represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train\\n %s' % X_train)\n",
    "print('X_test\\n%s' % X_test)\n",
    "# Even if sometimes your ML does not depend on ED, even then we need feature scaling as they will converge much fatser\n",
    "# like for DecisionTree ML algos; otherwise they will take very long to run.\n",
    "\n",
    "# And we dont need scaling on labels (or dependent variable), as its a categorical type will value as 0 or 1\n",
    "# in some cases we might need it (like regression models), but we'll see that later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
